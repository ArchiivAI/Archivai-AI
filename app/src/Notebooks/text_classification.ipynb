{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cohere\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom_id</th>\n",
       "      <th>image_url</th>\n",
       "      <th>markdown_text</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resume/0000153377.jpg</td>\n",
       "      <td>https://filstrg.blob.core.windows.net/tobacco-...</td>\n",
       "      <td>## Fitzmaurice, Mary Anne\\n\\n### Research Biol...</td>\n",
       "      <td>\\nFitzmaurice, Mary Anne\\nResearch Biologist\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Resume/10036815_10036823.jpg</td>\n",
       "      <td>https://filstrg.blob.core.windows.net/tobacco-...</td>\n",
       "      <td># CURRICULUM VITAE\\n\\n## Name: \\nPeter M. Howl...</td>\n",
       "      <td>CURRICULUM VITAE\\n\\nName: Peter M. Howley, M.D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Resume/10087799_10087801.jpg</td>\n",
       "      <td>https://filstrg.blob.core.windows.net/tobacco-...</td>\n",
       "      <td>![Form No. 1a (For N.I.H. Continuation Grant a...</td>\n",
       "      <td>Form No. 1a (For N.I.H. Continuation Grant app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Resume/10150247_10150256.jpg</td>\n",
       "      <td>https://filstrg.blob.core.windows.net/tobacco-...</td>\n",
       "      <td>### UNIVERSITY OF MIAMI\\n\\n#### CURRICULUM VIT...</td>\n",
       "      <td>UNIVERSITY OF MIAMI\\n\\nCURRICULUM VITAE\\n\\nSta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Resume/11300115-0116.jpg</td>\n",
       "      <td>https://filstrg.blob.core.windows.net/tobacco-...</td>\n",
       "      <td># CURRICULUM VITAE\\n\\n## WILLIAM CARSON HINDS\\...</td>\n",
       "      <td>CURRICULUM VITAE\\n\\nWILLIAM CARSON HINDS\\n\\nBo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      custom_id  \\\n",
       "0         Resume/0000153377.jpg   \n",
       "1  Resume/10036815_10036823.jpg   \n",
       "2  Resume/10087799_10087801.jpg   \n",
       "3  Resume/10150247_10150256.jpg   \n",
       "4      Resume/11300115-0116.jpg   \n",
       "\n",
       "                                           image_url  \\\n",
       "0  https://filstrg.blob.core.windows.net/tobacco-...   \n",
       "1  https://filstrg.blob.core.windows.net/tobacco-...   \n",
       "2  https://filstrg.blob.core.windows.net/tobacco-...   \n",
       "3  https://filstrg.blob.core.windows.net/tobacco-...   \n",
       "4  https://filstrg.blob.core.windows.net/tobacco-...   \n",
       "\n",
       "                                       markdown_text  \\\n",
       "0  ## Fitzmaurice, Mary Anne\\n\\n### Research Biol...   \n",
       "1  # CURRICULUM VITAE\\n\\n## Name: \\nPeter M. Howl...   \n",
       "2  ![Form No. 1a (For N.I.H. Continuation Grant a...   \n",
       "3  ### UNIVERSITY OF MIAMI\\n\\n#### CURRICULUM VIT...   \n",
       "4  # CURRICULUM VITAE\\n\\n## WILLIAM CARSON HINDS\\...   \n",
       "\n",
       "                                            raw_text  \n",
       "0  \\nFitzmaurice, Mary Anne\\nResearch Biologist\\n...  \n",
       "1  CURRICULUM VITAE\\n\\nName: Peter M. Howley, M.D...  \n",
       "2  Form No. 1a (For N.I.H. Continuation Grant app...  \n",
       "3  UNIVERSITY OF MIAMI\\n\\nCURRICULUM VITAE\\n\\nSta...  \n",
       "4  CURRICULUM VITAE\\n\\nWILLIAM CARSON HINDS\\n\\nBo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_path = os.path.join(os.getcwd(), '..', 'Datasets\\\\extracted_data.csv')\n",
    "data = pd.read_csv(text_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the document class from the custom_id\n",
    "data['class'] = data['custom_id'].apply(lambda x : x.split('/')[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "Memo          620\n",
       "Email         596\n",
       "Letter        566\n",
       "Form          430\n",
       "Report        265\n",
       "scientific    258\n",
       "ADVE          222\n",
       "Note          200\n",
       "News          188\n",
       "Resume        120\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3465, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_id        0\n",
       "image_url        0\n",
       "markdown_text    1\n",
       "raw_text         1\n",
       "class            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping the empty document\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "\n",
    "co_embed = cohere.Client(\n",
    "    api_key=os.getenv(\"AZURE_ML_COHERE_EMBED_CREDENTIAL\"),\n",
    "    base_url=os.getenv(\"AZURE_ML_COHERE_EMBED_ENDPOINT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the embedding\n",
    "docs = data.head()[\"raw_text\"].tolist()\n",
    "embeddings = co_embed.embed(\n",
    "    input_type=\"classification\",\n",
    "    texts=docs,\n",
    ").embeddings\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data to train and test\n",
    "docs_train, docs_test, classes_train, classes_test = train_test_split(\n",
    "            list(data['raw_text']), list(data['class']), test_size=0.20, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating raw text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the train embeddings \n",
    "embeddings_train = co_embed.embed(texts=docs_train,\n",
    "                            input_type='classification'\n",
    "                            ).embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 2598)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_train[0]), len(embeddings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test embeddings\n",
    "embeddings_test = co_embed.embed(texts=docs_test,\n",
    "                           input_type='classification'\n",
    "                            ).embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 866)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_test[0]), len(embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the target variable \n",
    "encoder = LabelEncoder()\n",
    "\n",
    "classes_train_encoded = encoder.fit_transform(classes_train)\n",
    "classes_test_encoded = encoder.transform(classes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_tensor = torch.tensor(embeddings_train, dtype=torch.float32)\n",
    "classes_train_tensor = torch.tensor(classes_train_encoded, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "docs_test_tensor = torch.tensor(embeddings_test, dtype=torch.float32)\n",
    "classes_test_tensor = torch.tensor(classes_test_encoded, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            # layer 1\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            # layer 2\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            # output layer\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextClassifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = data['class'].value_counts().sort_index()\n",
    "weights = 1.0 / torch.tensor(class_counts.values, dtype=torch.float32)\n",
    "weights = weights / weights.sum()\n",
    "criterion = nn.CrossEntropyLoss(weight=weights.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ArchivAI\\ArchivAI\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(docs_train_tensor, classes_train_tensor)\n",
    "test_dataset = TensorDataset(docs_test_tensor, classes_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]\n",
      "Training - Loss: 2.0243, Accuracy: 26.64%\n",
      "Validation - Loss: 1.7439, Accuracy: 57.04%\n",
      "Epoch [2/30]\n",
      "Training - Loss: 1.4874, Accuracy: 51.62%\n",
      "Validation - Loss: 1.2616, Accuracy: 67.67%\n",
      "Epoch [3/30]\n",
      "Training - Loss: 1.1988, Accuracy: 61.86%\n",
      "Validation - Loss: 1.0725, Accuracy: 72.52%\n",
      "Epoch [4/30]\n",
      "Training - Loss: 1.0152, Accuracy: 68.32%\n",
      "Validation - Loss: 0.9296, Accuracy: 74.48%\n",
      "Epoch [5/30]\n",
      "Training - Loss: 0.8744, Accuracy: 72.48%\n",
      "Validation - Loss: 0.8452, Accuracy: 78.18%\n",
      "Epoch [6/30]\n",
      "Training - Loss: 0.7829, Accuracy: 76.79%\n",
      "Validation - Loss: 0.7786, Accuracy: 77.37%\n",
      "Epoch [7/30]\n",
      "Training - Loss: 0.6676, Accuracy: 80.22%\n",
      "Validation - Loss: 0.7243, Accuracy: 80.60%\n",
      "Epoch [8/30]\n",
      "Training - Loss: 0.6068, Accuracy: 83.26%\n",
      "Validation - Loss: 0.6844, Accuracy: 80.37%\n",
      "Epoch [9/30]\n",
      "Training - Loss: 0.5482, Accuracy: 84.45%\n",
      "Validation - Loss: 0.6574, Accuracy: 80.72%\n",
      "Epoch [10/30]\n",
      "Training - Loss: 0.4880, Accuracy: 85.95%\n",
      "Validation - Loss: 0.6362, Accuracy: 81.41%\n",
      "Epoch [11/30]\n",
      "Training - Loss: 0.4442, Accuracy: 86.14%\n",
      "Validation - Loss: 0.6214, Accuracy: 81.87%\n",
      "Epoch [12/30]\n",
      "Training - Loss: 0.3859, Accuracy: 89.34%\n",
      "Validation - Loss: 0.6093, Accuracy: 82.33%\n",
      "Epoch [13/30]\n",
      "Training - Loss: 0.3581, Accuracy: 90.18%\n",
      "Validation - Loss: 0.5930, Accuracy: 82.22%\n",
      "Epoch [14/30]\n",
      "Training - Loss: 0.3413, Accuracy: 90.11%\n",
      "Validation - Loss: 0.5872, Accuracy: 82.10%\n",
      "Epoch [15/30]\n",
      "Training - Loss: 0.2977, Accuracy: 91.80%\n",
      "Validation - Loss: 0.5909, Accuracy: 82.68%\n",
      "Epoch [16/30]\n",
      "Training - Loss: 0.2457, Accuracy: 94.00%\n",
      "Validation - Loss: 0.5804, Accuracy: 83.49%\n",
      "Epoch [17/30]\n",
      "Training - Loss: 0.2458, Accuracy: 93.46%\n",
      "Validation - Loss: 0.5753, Accuracy: 82.91%\n",
      "Epoch [18/30]\n",
      "Training - Loss: 0.2185, Accuracy: 93.76%\n",
      "Validation - Loss: 0.5753, Accuracy: 82.91%\n",
      "Epoch [19/30]\n",
      "Training - Loss: 0.1773, Accuracy: 95.80%\n",
      "Validation - Loss: 0.5722, Accuracy: 82.45%\n",
      "Epoch [20/30]\n",
      "Training - Loss: 0.1958, Accuracy: 94.80%\n",
      "Validation - Loss: 0.5826, Accuracy: 82.79%\n",
      "Epoch [21/30]\n",
      "Training - Loss: 0.1683, Accuracy: 96.11%\n",
      "Validation - Loss: 0.5812, Accuracy: 83.26%\n",
      "Epoch [22/30]\n",
      "Training - Loss: 0.1411, Accuracy: 96.07%\n",
      "Validation - Loss: 0.5811, Accuracy: 83.60%\n",
      "Epoch [23/30]\n",
      "Training - Loss: 0.1249, Accuracy: 97.27%\n",
      "Validation - Loss: 0.5994, Accuracy: 83.14%\n",
      "Epoch [24/30]\n",
      "Training - Loss: 0.1174, Accuracy: 97.07%\n",
      "Validation - Loss: 0.5952, Accuracy: 83.03%\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device, dtype=torch.long).squeeze()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device, dtype=torch.long).squeeze()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "            val_total += targets.size(0)\n",
    "    \n",
    "    val_loss = val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# Load best model for final evaluation\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArchivAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
